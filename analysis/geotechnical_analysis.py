#!/usr/bin/env python3
"""
Fonctions d'analyse g√©otechnique avanc√©es pour CPT/CPTU
Analyse 3D des couches, classification d√©taill√©e des sols, g√©olocalisation
"""

import pandas as pd
import numpy as np
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy import stats
from scipy.signal import find_peaks, savgol_filter
from scipy.spatial import Delaunay
from scipy.interpolate import griddata
import warnings
warnings.filterwarnings('ignore')

# Import GeoPandas pour la g√©olocalisation
try:
    import geopandas as gpd
    from shapely.geometry import Point
    GEOPANDAS_AVAILABLE = True
except ImportError:
    GEOPANDAS_AVAILABLE = False
    st.warning("‚ö†Ô∏è GeoPandas non install√©. G√©olocalisation limit√©e.")

def estimate_soil_type(df):
    """Classification d√©taill√©e des sols bas√©e sur Robertson (1990) et Schmertmann (1978)"""
    def classify_soil_detailed(qc, fs, depth):
        if pd.isna(qc) or pd.isna(fs) or qc <= 0 or fs < 0:
            return 'Inconnu', 'Unknown', 0, 0, 0

        # Calcul des indices de Robertson
        fr = (fs / qc) * 100  # Friction ratio (%)
        Ic = ((3.47 - np.log10(qc))**2 + (np.log10(fr) + 1.22)**2)**0.5  # Soil Behavior Type Index

        # Classification d√©taill√©e selon Robertson (1986)
        if Ic < 1.31:
            if fr < 0.5:
                soil_type = 'Sable graveleux tr√®s dense'
                soil_class = 'Gravel'
                color = '#8B4513'  # Brown
            else:
                soil_type = 'Sable silteux dense'
                soil_class = 'Sand'
                color = '#DAA520'  # Goldenrod
        elif Ic < 2.05:
            if fr < 1:
                soil_type = 'Sable dense √† tr√®s dense'
                soil_class = 'Sand'
                color = '#FFD700'  # Gold
            else:
                soil_type = 'Sable silteux'
                soil_class = 'Sandy Silt'
                color = '#F0E68C'  # Khaki
        elif Ic < 2.60:
            if fr < 2:
                soil_type = 'Sable l√¢che √† compact'
                soil_class = 'Sand'
                color = '#FFE4B5'  # Moccasin
            else:
                soil_type = 'Silt sableux'
                soil_class = 'Silty Sand'
                color = '#DEB887'  # Burlywood
        elif Ic < 2.95:
            if fr < 4:
                soil_type = 'Silt argileux'
                soil_class = 'Clayey Silt'
                color = '#D2B48C'  # Tan
            else:
                soil_type = 'Argile silteuse'
                soil_class = 'Silty Clay'
                color = '#BC8F8F'  # Rosy Brown
        elif Ic < 3.60:
            soil_type = 'Argile'
            soil_class = 'Clay'
            color = '#CD853F'  # Peru
        else:
            soil_type = 'Argile organique/molte'
            soil_class = 'Organic Clay'
            color = '#A0522D'  # Sienna

        # Ajustements bas√©s sur la profondeur (diagen√®se)
        if depth > 20:  # Sols profonds plus consolid√©s
            if 'Sable' in soil_type and qc > 15:
                soil_type += ' (ciment√©)'
            elif 'Argile' in soil_type and qc > 8:
                soil_type += ' (consolid√©e)'

        return soil_type, soil_class, Ic, fr, color

    df_copy = df.copy()
    results = df_copy.apply(lambda row: classify_soil_detailed(row['qc'], row['fs'], row['Depth']), axis=1)

    df_copy['Soil_Type_Detailed'] = [r[0] for r in results]
    df_copy['Soil_Class'] = [r[1] for r in results]
    df_copy['Ic'] = [r[2] for r in results]
    df_copy['Fr'] = [r[3] for r in results]
    df_copy['Soil_Color'] = [r[4] for r in results]

    # Classification simplifi√©e pour compatibilit√©
    df_copy['Soil_Type'] = df_copy['Soil_Type_Detailed'].map({
        'Sable graveleux tr√®s dense': 'Sable dense',
        'Sable silteux dense': 'Sable dense',
        'Sable dense √† tr√®s dense': 'Sable dense',
        'Sable silteux': 'Sable',
        'Sable l√¢che √† compact': 'Sable',
        'Silt sableux': 'Sable',
        'Silt argileux': 'Limon',
        'Argile silteuse': 'Argile',
        'Argile': 'Argile',
        'Argile organique/molte': 'Argile molle'
    }).fillna('Inconnu')

    return df_copy

def calculate_crr(df):
    """Calcule le Cyclic Resistance Ratio (CRR) avec analyse avanc√©e"""
    df_copy = df.copy()

    # Estimation de la contrainte verticale effective
    gamma = 18  # Poids volumique moyen (kN/m¬≥)
    sigma_v = df_copy['Depth'] * gamma  # Contrainte verticale totale
    sigma_vo = sigma_v * 0.5  # Estimation simplifi√©e de la contrainte effective

    # Normalisation qc1N selon Robertson & Wride (1998)
    df_copy['qc1N'] = df_copy['qc'] * (100 / sigma_vo)**0.5

    # CRR selon Idriss & Boulanger (2008) pour magnitude 7.5
    df_copy['CRR'] = np.exp(df_copy['qc1N']/113 + (df_copy['qc1N']/1000)**2 - 3.5) / 2.36

    # Facteur de s√©curit√© contre la liqu√©faction
    df_copy['FS_Liquefaction'] = df_copy['CRR'] / 0.3  # CSR = 0.3 pour M=7.5

    # Classification du risque de liqu√©faction
    df_copy['Liquefaction_Risk'] = pd.cut(df_copy['FS_Liquefaction'],
                                         bins=[0, 1, 1.2, 1.5, np.inf],
                                         labels=['Tr√®s √©lev√©', '√âlev√©', 'Mod√©r√©', 'Faible'])

    return df_copy

def identify_soil_layers_3d(df, min_thickness=0.5):
    """Identifie les couches g√©ologiques en 3D avec √©paisseurs et transitions"""
    layers = []
    current_layer_start = df['Depth'].min()
    current_soil_type = df.iloc[0]['Soil_Type_Detailed']
    current_color = df.iloc[0]['Soil_Color']

    for i in range(1, len(df)):
        if df.iloc[i]['Soil_Type_Detailed'] != current_soil_type:
            # Fin de couche d√©tect√©e
            thickness = df.iloc[i-1]['Depth'] - current_layer_start
            if thickness >= min_thickness:
                layers.append({
                    'start_depth': current_layer_start,
                    'end_depth': df.iloc[i-1]['Depth'],
                    'thickness': thickness,
                    'soil_type': current_soil_type,
                    'soil_class': df.iloc[i-1]['Soil_Class'],
                    'color': current_color,
                    'avg_qc': df.iloc[i-1]['qc'],
                    'avg_fs': df.iloc[i-1]['fs'],
                    'avg_Ic': df.iloc[i-1]['Ic']
                })
            current_layer_start = df.iloc[i]['Depth']
            current_soil_type = df.iloc[i]['Soil_Type_Detailed']
            current_color = df.iloc[i]['Soil_Color']

    # Derni√®re couche
    thickness = df.iloc[-1]['Depth'] - current_layer_start
    if thickness >= min_thickness:
        layers.append({
            'start_depth': current_layer_start,
            'end_depth': df.iloc[-1]['Depth'],
            'thickness': thickness,
            'soil_type': current_soil_type,
            'soil_class': df.iloc[-1]['Soil_Class'],
            'color': current_color,
            'avg_qc': df.iloc[-1]['qc'],
            'avg_fs': df.iloc[-1]['fs'],
            'avg_Ic': df.iloc[-1]['Ic']
        })

    return pd.DataFrame(layers)

def create_geospatial_analysis(df, lat=48.8566, lon=2.3522):
    """Cr√©e une analyse g√©ospatiale avec GeoPandas"""
    if not GEOPANDAS_AVAILABLE:
        st.warning("GeoPandas non disponible. Analyse g√©ospatiale limit√©e.")
        return None

    try:
        # Cr√©er des points g√©ographiques (simulation autour d'un point central)
        np.random.seed(42)
        n_points = len(df)

        # Distribution gaussienne autour du point central
        lats = np.random.normal(lat, 0.01, n_points)  # ~1km d'√©cart
        lons = np.random.normal(lon, 0.01, n_points)

        # Cr√©er GeoDataFrame
        geometry = [Point(xy) for xy in zip(lons, lats)]
        gdf = gpd.GeoDataFrame(df.copy(), geometry=geometry, crs='EPSG:4326')

        # Ajouter des attributs g√©ographiques
        gdf['latitude'] = lats
        gdf['longitude'] = lons
        gdf['elevation'] = 50 + np.random.normal(0, 5, n_points)  # √âl√©vation simul√©e

        return gdf

    except Exception as e:
        st.error(f"Erreur lors de l'analyse g√©ospatiale: {e}")
        return None

def create_advanced_visualizations(df, layers_df=None, gdf=None):
    """Cr√©e 10+ graphiques et tableaux avanc√©s pour l'analyse g√©otechnique"""

    visualizations = {}

    # 1. Profil 3D des couches g√©ologiques
    if layers_df is not None:
        fig_3d_layers = go.Figure()

        for _, layer in layers_df.iterrows():
            fig_3d_layers.add_trace(go.Scatter3d(
                x=[0, 1, 1, 0, 0],
                y=[layer['start_depth'], layer['start_depth'], layer['end_depth'], layer['end_depth'], layer['start_depth']],
                z=[0, 0, 0, 0, 0],
                mode='lines',
                line=dict(color=layer['color'], width=10),
                name=f"{layer['soil_type']} ({layer['thickness']:.1f}m)",
                showlegend=True
            ))

        fig_3d_layers.update_layout(
            title="Profil 3D des Couches G√©ologiques",
            scene=dict(
                xaxis_title="Position X",
                yaxis_title="Profondeur (m)",
                zaxis_title="Position Z"
            )
        )
        visualizations['3d_layers'] = fig_3d_layers

    # 2. Carte de chaleur Ic vs Profondeur
    fig_ic_heatmap = go.Figure(data=go.Heatmap(
        z=df['Ic'],
        x=df['Depth'],
        y=df['qc'],
        colorscale='Viridis',
        name='Indice Ic'
    ))
    fig_ic_heatmap.update_layout(
        title="Carte de Chaleur - Indice Ic vs Profondeur",
        xaxis_title="Profondeur (m)",
        yaxis_title="qc (MPa)"
    )
    visualizations['ic_heatmap'] = fig_ic_heatmap

    # 3. Histogramme des types de sols avec distribution
    soil_counts = df['Soil_Type_Detailed'].value_counts()
    fig_soil_dist = px.bar(
        x=soil_counts.index,
        y=soil_counts.values,
        color=soil_counts.index,
        title="Distribution des Types de Sols D√©taill√©s"
    )
    fig_soil_dist.update_layout(xaxis_title="Type de Sol", yaxis_title="Nombre d'√©chantillons")
    visualizations['soil_distribution'] = fig_soil_dist

    # 4. Graphique radar des propri√©t√©s moyennes par couche
    if layers_df is not None:
        categories = ['√âpaisseur', 'qc moyen', 'fs moyen', 'Ic moyen']

        fig_radar = go.Figure()

        for _, layer in layers_df.iterrows():
            values = [
                layer['thickness'],
                layer['avg_qc'],
                layer['avg_fs'],
                layer['avg_Ic']
            ]
            fig_radar.add_trace(go.Scatterpolar(
                r=values,
                theta=categories,
                fill='toself',
                name=f"{layer['soil_type'][:20]}..."
            ))

        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=True)),
            title="Propri√©t√©s Moyennes par Couche G√©ologique"
        )
        visualizations['radar_properties'] = fig_radar

    # 5. Analyse de tendance avec lissage
    window_size = min(21, len(df) // 2 * 2 + 1)  # Taille impaire
    df_smooth = df.copy()
    df_smooth['qc_smooth'] = savgol_filter(df['qc'], window_size, 3)
    df_smooth['fs_smooth'] = savgol_filter(df['fs'], window_size, 3)

    fig_trends = make_subplots(rows=2, cols=1, shared_xaxes=True,
                              subplot_titles=['R√©sistance de Pointe (qc)', 'Frottement de Manche (fs)'])

    fig_trends.add_trace(go.Scatter(x=df['Depth'], y=df['qc'], mode='markers', name='qc brut',
                                   marker=dict(size=3, color='lightblue')), row=1, col=1)
    fig_trends.add_trace(go.Scatter(x=df_smooth['Depth'], y=df_smooth['qc_smooth'],
                                   mode='lines', name='qc liss√©', line=dict(color='blue', width=2)), row=1, col=1)

    fig_trends.add_trace(go.Scatter(x=df['Depth'], y=df['fs'], mode='markers', name='fs brut',
                                   marker=dict(size=3, color='lightcoral')), row=2, col=1)
    fig_trends.add_trace(go.Scatter(x=df_smooth['Depth'], y=df_smooth['fs_smooth'],
                                   mode='lines', name='fs liss√©', line=dict(color='red', width=2)), row=2, col=1)

    fig_trends.update_layout(title="Analyse de Tendance avec Lissage Savitzky-Golay")
    visualizations['trend_analysis'] = fig_trends

    # 6. Diagramme de dispersion qc/fs color√© par type de sol
    fig_scatter = px.scatter(df, x='qc', y='fs', color='Soil_Type_Detailed',
                            title="Corr√©lation qc/fs par Type de Sol",
                            labels={'qc': 'R√©sistance de Pointe (MPa)', 'fs': 'Frottement de Manche (MPa)'})
    visualizations['correlation_scatter'] = fig_scatter

    # 7. Profil de risque de liqu√©faction
    if 'FS_Liquefaction' in df.columns:
        fig_liq = make_subplots(rows=1, cols=2,
                               subplot_titles=['Facteur de S√©curit√©', 'Risque de Liqu√©faction'])

        fig_liq.add_trace(go.Scatter(x=df['FS_Liquefaction'], y=df['Depth'], mode='lines+markers',
                                    name='FS', line=dict(color='red')), row=1, col=1)

        risk_colors = {'Tr√®s √©lev√©': 'darkred', '√âlev√©': 'red', 'Mod√©r√©': 'orange', 'Faible': 'green'}
        for risk in df['Liquefaction_Risk'].unique():
            mask = df['Liquefaction_Risk'] == risk
            fig_liq.add_trace(go.Scatter(
                x=df[mask]['Depth'],
                y=[1] * mask.sum(),
                mode='markers',
                marker=dict(color=risk_colors.get(risk, 'gray'), size=10),
                name=risk
            ), row=1, col=2)

        fig_liq.update_layout(title="Analyse du Risque de Liqu√©faction")
        visualizations['liquefaction_profile'] = fig_liq

    # 8. Statistiques descriptives par couche
    if layers_df is not None:
        stats_data = []
        for _, layer in layers_df.iterrows():
            mask = (df['Depth'] >= layer['start_depth']) & (df['Depth'] <= layer['end_depth'])
            layer_data = df[mask]

            stats_data.append({
                'Couche': layer['soil_type'][:30],
                '√âpaisseur (m)': f"{layer['thickness']:.1f}",
                'Profondeur (m)': f"{layer['start_depth']:.1f}-{layer['end_depth']:.1f}",
                'qc moyen (MPa)': f"{layer_data['qc'].mean():.1f}",
                'qc min-max (MPa)': f"{layer_data['qc'].min():.1f}-{layer_data['qc'].max():.1f}",
                'fs moyen (MPa)': f"{layer_data['fs'].mean():.1f}",
                'Ic moyen': f"{layer_data['Ic'].mean():.2f}",
                '√âchantillons': len(layer_data)
            })

        stats_df = pd.DataFrame(stats_data)
        visualizations['layer_statistics'] = stats_df

    # 9. Analyse fr√©quentielle (FFT) des variations
    if len(df) > 32:  # Minimum pour FFT
        qc_fft = np.fft.fft(df['qc'].values)
        freqs = np.fft.fftfreq(len(df), d=(df['Depth'].diff().mean()))

        fig_fft = make_subplots(rows=1, cols=2,
                               subplot_titles=['Spectre de Fr√©quence qc', 'P√©riodogramme'])

        fig_fft.add_trace(go.Scatter(x=freqs[:len(freqs)//2], y=np.abs(qc_fft)[:len(qc_fft)//2],
                                    mode='lines', name='Amplitude'), row=1, col=1)

        fig_fft.add_trace(go.Scatter(x=df['Depth'], y=df['qc'], mode='lines', name='Signal original'), row=1, col=2)
        fig_fft.add_trace(go.Scatter(x=df['Depth'], y=savgol_filter(df['qc'], 11, 3),
                                    mode='lines', name='Tendance', line=dict(dash='dash')), row=1, col=2)

        fig_fft.update_layout(title="Analyse Fr√©quentielle des Variations de qc")
        visualizations['frequency_analysis'] = fig_fft

    # 10. Remplacement de la carte g√©ographique par analyse de zones CPTU
    try:
        # Cr√©er des coordonn√©es simul√©es pour les zones CPTU
        np.random.seed(42)
        n_zones = min(10, len(df))  # Maximum 10 zones
        zone_centers = []

        # Cr√©er des centres de zones distribu√©s
        for i in range(n_zones):
            angle = 2 * np.pi * i / n_zones
            radius = 50 + np.random.uniform(-20, 20)
            x = radius * np.cos(angle)
            y = radius * np.sin(angle)
            zone_centers.append((x, y))

        # Assigner chaque point √† une zone
        df_zones = df.copy()
        df_zones['zone_id'] = np.random.randint(0, n_zones, len(df))
        df_zones['zone_x'] = df_zones['zone_id'].map(lambda i: zone_centers[i][0])
        df_zones['zone_y'] = df_zones['zone_id'].map(lambda i: zone_centers[i][1])

        # Graphique 3D des zones CPTU
        fig_zones_3d = go.Figure()

        for zone_id in range(n_zones):
            zone_data = df_zones[df_zones['zone_id'] == zone_id]
            if not zone_data.empty:
                fig_zones_3d.add_trace(go.Scatter3d(
                    x=zone_data['zone_x'],
                    y=zone_data['zone_y'],
                    z=zone_data['Depth'],
                    mode='markers',
                    name=f'Zone {zone_id + 1}',
                    marker=dict(
                        size=6,
                        color=zone_data['qc'],
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="qc (MPa)")
                    ),
                    text=[f"Zone {zone_id + 1}<br>Profondeur: {d:.1f}m<br>qc: {q:.1f}MPa"
                          for d, q in zip(zone_data['Depth'], zone_data['qc'])]
                ))

        fig_zones_3d.update_layout(
            title="Zones CPTU 3D avec Distribution Spatiale",
            scene=dict(
                xaxis_title="Position X (m)",
                yaxis_title="Position Y (m)",
                zaxis_title="Profondeur (m)",
                zaxis=dict(autorange="reversed")
            )
        )
        visualizations['cptu_zones_3d'] = fig_zones_3d

    except Exception as e:
        st.warning(f"Erreur lors de la cr√©ation des zones CPTU: {e}")

    # === 10 NOUVEAUX GRAPHIQUES 3D AVEC TRIANGULATION ===

    # 11. Surface triangul√©e 3D des types de sol
    try:
        from scipy.spatial import Delaunay

        # Cr√©er une grille de points pour la triangulation
        x_grid = np.linspace(df['Depth'].min(), df['Depth'].max(), 20)
        y_grid = np.linspace(0, 100, 20)  # Position lat√©rale simul√©e
        X, Y = np.meshgrid(x_grid, y_grid)
        X = X.flatten()
        Y = Y.flatten()

        # Interpoler les valeurs de qc sur la grille
        from scipy.interpolate import griddata
        qc_interp = griddata(
            (df['Depth'], np.random.uniform(0, 100, len(df))),
            df['qc'],
            (X, Y),
            method='linear'
        )

        # Triangulation
        points = np.column_stack([X, Y])
        tri = Delaunay(points)

        # Cr√©er la surface 3D triangul√©e
        fig_triangulated = go.Figure()

        fig_triangulated.add_trace(go.Mesh3d(
            x=X,
            y=Y,
            z=qc_interp,
            i=tri.simplices[:, 0],
            j=tri.simplices[:, 1],
            k=tri.simplices[:, 2],
            opacity=0.8,
            color='lightblue',
            name='Surface qc'
        ))

        # Ajouter les points de donn√©es r√©els
        fig_triangulated.add_trace(go.Scatter3d(
            x=df['Depth'],
            y=np.random.uniform(0, 100, len(df)),
            z=df['qc'],
            mode='markers',
            marker=dict(size=4, color='red', opacity=0.7),
            name='Points r√©els'
        ))

        fig_triangulated.update_layout(
            title="Surface Triangul√©e 3D - R√©sistance de Pointe (qc)",
            scene=dict(
                xaxis_title="Profondeur (m)",
                yaxis_title="Position Lat√©rale (m)",
                zaxis_title="qc (MPa)"
            )
        )
        visualizations['triangulated_surface_qc'] = fig_triangulated

    except Exception as e:
        st.warning(f"Erreur lors de la triangulation qc: {e}")

    # 12. Surface triangul√©e Ic vs Profondeur
    try:
        ic_interp = griddata(
            (df['Depth'], np.random.uniform(0, 100, len(df))),
            df['Ic'],
            (X, Y),
            method='linear'
        )

        fig_triangulated_ic = go.Figure()

        fig_triangulated_ic.add_trace(go.Mesh3d(
            x=X,
            y=Y,
            z=ic_interp,
            i=tri.simplices[:, 0],
            j=tri.simplices[:, 1],
            k=tri.simplices[:, 2],
            opacity=0.8,
            colorscale='Viridis',
            intensity=ic_interp,
            name='Surface Ic'
        ))

        fig_triangulated_ic.add_trace(go.Scatter3d(
            x=df['Depth'],
            y=np.random.uniform(0, 100, len(df)),
            z=df['Ic'],
            mode='markers',
            marker=dict(size=4, color='red', opacity=0.7),
            name='Points r√©els'
        ))

        fig_triangulated_ic.update_layout(
            title="Surface Triangul√©e 3D - Indice Ic (Soil Behavior Type)",
            scene=dict(
                xaxis_title="Profondeur (m)",
                yaxis_title="Position Lat√©rale (m)",
                zaxis_title="Indice Ic"
            )
        )
        visualizations['triangulated_surface_ic'] = fig_triangulated_ic

    except Exception as e:
        st.warning(f"Erreur lors de la triangulation Ic: {e}")

    # 13. Volume 3D des couches g√©ologiques avec triangulation
    try:
        if layers_df is not None and not layers_df.empty:
            fig_layers_volume = go.Figure()

            colors = ['#8B4513', '#DAA520', '#F4A460', '#DEB887', '#D2B48C', '#BC8F8F']

            for idx, layer in layers_df.iterrows():
                # Cr√©er une surface pour chaque couche
                layer_mask = (df['Depth'] >= layer['start_depth']) & (df['Depth'] <= layer['end_depth'])
                layer_data = df[layer_mask]

                if not layer_data.empty:
                    # Points pour la couche
                    x_layer = np.random.uniform(0, 100, len(layer_data))
                    y_layer = layer_data['Depth']
                    z_layer = np.random.uniform(0, 50, len(layer_data))  # √âpaisseur simul√©e

                    # Triangulation pour la couche
                    if len(layer_data) >= 3:
                        points_layer = np.column_stack([x_layer, y_layer])
                        tri_layer = Delaunay(points_layer)

                        fig_layers_volume.add_trace(go.Mesh3d(
                            x=x_layer,
                            y=y_layer,
                            z=z_layer,
                            i=tri_layer.simplices[:, 0],
                            j=tri_layer.simplices[:, 1],
                            k=tri_layer.simplices[:, 2],
                            opacity=0.7,
                            color=colors[idx % len(colors)],
                            name=f"{layer['soil_type'][:20]}..."
                        ))

            fig_layers_volume.update_layout(
                title="Volume 3D Triangul√© des Couches G√©ologiques",
                scene=dict(
                    xaxis_title="Position X (m)",
                    yaxis_title="Profondeur (m)",
                    zaxis_title="√âpaisseur (m)"
                )
            )
            visualizations['layers_volume_3d'] = fig_layers_volume

    except Exception as e:
        st.warning(f"Erreur lors de la cr√©ation du volume 3D: {e}")

    # 14. Surface 3D des risques de liqu√©faction
    try:
        if 'FS_Liquefaction' in df.columns:
            fs_interp = griddata(
                (df['Depth'], np.random.uniform(0, 100, len(df))),
                df['FS_Liquefaction'],
                (X, Y),
                method='linear'
            )

            fig_liquefaction_3d = go.Figure()

            fig_liquefaction_3d.add_trace(go.Mesh3d(
                x=X,
                y=Y,
                z=fs_interp,
                i=tri.simplices[:, 0],
                j=tri.simplices[:, 1],
                k=tri.simplices[:, 2],
                opacity=0.8,
                colorscale='RdYlGn',
                intensity=fs_interp,
                name='FS Liquefaction'
            ))

            # Colorer selon le risque
            risk_colors = []
            for fs in df['FS_Liquefaction']:
                if fs < 1.2:
                    risk_colors.append('red')
                elif fs < 1.5:
                    risk_colors.append('orange')
                else:
                    risk_colors.append('green')

            fig_liquefaction_3d.add_trace(go.Scatter3d(
                x=df['Depth'],
                y=np.random.uniform(0, 100, len(df)),
                z=df['FS_Liquefaction'],
                mode='markers',
                marker=dict(size=6, color=risk_colors, opacity=0.8),
                name='Points de risque'
            ))

            fig_liquefaction_3d.update_layout(
                title="Surface 3D Triangul√©e - Risque de Liqu√©faction",
                scene=dict(
                    xaxis_title="Profondeur (m)",
                    yaxis_title="Position Lat√©rale (m)",
                    zaxis_title="FS Liquefaction"
                )
            )
            visualizations['liquefaction_surface_3d'] = fig_liquefaction_3d

    except Exception as e:
        st.warning(f"Erreur lors de la surface de liqu√©faction: {e}")

    # 15. Topographie 3D des clusters
    try:
        if 'Cluster' in df.columns:
            n_clusters = df['Cluster'].max() + 1
            fig_clusters_3d = go.Figure()

            for cluster_id in range(n_clusters):
                cluster_data = df[df['Cluster'] == cluster_id]

                if not cluster_data.empty:
                    # Triangulation par cluster
                    x_cluster = np.random.uniform(0, 100, len(cluster_data))
                    y_cluster = cluster_data['Depth']
                    z_cluster = cluster_data['qc']

                    if len(cluster_data) >= 3:
                        points_cluster = np.column_stack([x_cluster, y_cluster])
                        tri_cluster = Delaunay(points_cluster)

                        fig_clusters_3d.add_trace(go.Mesh3d(
                            x=x_cluster,
                            y=y_cluster,
                            z=z_cluster,
                            i=tri_cluster.simplices[:, 0],
                            j=tri_cluster.simplices[:, 1],
                            k=tri_cluster.simplices[:, 2],
                            opacity=0.6,
                            name=f'Cluster {cluster_id}'
                        ))

            fig_clusters_3d.update_layout(
                title="Topographie 3D Triangul√©e par Clusters",
                scene=dict(
                    xaxis_title="Position X (m)",
                    yaxis_title="Profondeur (m)",
                    zaxis_title="qc (MPa)"
                )
            )
            visualizations['clusters_topography_3d'] = fig_clusters_3d

    except Exception as e:
        st.warning(f"Erreur lors de la topographie des clusters: {e}")

    # 16. Structure 3D des types de sol d√©taill√©s
    try:
        fig_soil_structure = go.Figure()

        soil_types = df['Soil_Type_Detailed'].unique()
        colors_soil = px.colors.qualitative.Set3

        for idx, soil_type in enumerate(soil_types):
            soil_data = df[df['Soil_Type_Detailed'] == soil_type]

            if not soil_data.empty and len(soil_data) >= 3:
                x_soil = np.random.uniform(0, 100, len(soil_data))
                y_soil = soil_data['Depth']
                z_soil = soil_data['qc']

                points_soil = np.column_stack([x_soil, y_soil])
                tri_soil = Delaunay(points_soil)

                fig_soil_structure.add_trace(go.Mesh3d(
                    x=x_soil,
                    y=y_soil,
                    z=z_soil,
                    i=tri_soil.simplices[:, 0],
                    j=tri_soil.simplices[:, 1],
                    k=tri_soil.simplices[:, 2],
                    opacity=0.7,
                    color=colors_soil[idx % len(colors_soil)],
                    name=f"{soil_type[:15]}..."
                ))

        fig_soil_structure.update_layout(
            title="Structure 3D Triangul√©e des Types de Sol D√©taill√©s",
            scene=dict(
                xaxis_title="Position X (m)",
                yaxis_title="Profondeur (m)",
                zaxis_title="qc (MPa)"
            )
        )
        visualizations['soil_structure_3d'] = fig_soil_structure

    except Exception as e:
        st.warning(f"Erreur lors de la structure des sols: {e}")

    # 17. Gradient 3D de propri√©t√©s m√©caniques
    try:
        fig_gradient_3d = go.Figure()

        # Calculer le gradient de qc
        qc_gradient = np.gradient(df['qc'].values, df['Depth'].values)

        gradient_interp = griddata(
            (df['Depth'], np.random.uniform(0, 100, len(df))),
            qc_gradient,
            (X, Y),
            method='linear'
        )

        fig_gradient_3d.add_trace(go.Mesh3d(
            x=X,
            y=Y,
            z=gradient_interp,
            i=tri.simplices[:, 0],
            j=tri.simplices[:, 1],
            k=tri.simplices[:, 2],
            opacity=0.8,
            colorscale='RdBu',
            intensity=gradient_interp,
            name='Gradient qc'
        ))

        fig_gradient_3d.update_layout(
            title="Gradient 3D Triangul√© des Propri√©t√©s M√©caniques",
            scene=dict(
                xaxis_title="Profondeur (m)",
                yaxis_title="Position Lat√©rale (m)",
                zaxis_title="Gradient qc (MPa/m)"
            )
        )
        visualizations['gradient_3d'] = fig_gradient_3d

    except Exception as e:
        st.warning(f"Erreur lors du gradient 3D: {e}")

    # 18. Iso-surfaces 3D des param√®tres g√©otechniques
    try:
        fig_isosurface = go.Figure()

        # Cr√©er des isosurfaces pour diff√©rentes valeurs de qc
        qc_values = np.linspace(df['qc'].min(), df['qc'].max(), 5)

        for qc_val in qc_values:
            mask = df['qc'] >= qc_val
            if mask.sum() >= 4:  # Assez de points pour triangulation
                iso_data = df[mask]
                x_iso = np.random.uniform(0, 100, len(iso_data))
                y_iso = iso_data['Depth']
                z_iso = np.full(len(iso_data), qc_val)

                if len(iso_data) >= 3:
                    points_iso = np.column_stack([x_iso, y_iso])
                    tri_iso = Delaunay(points_iso)

                    fig_isosurface.add_trace(go.Mesh3d(
                        x=x_iso,
                        y=y_iso,
                        z=z_iso,
                        i=tri_iso.simplices[:, 0],
                        j=tri_iso.simplices[:, 1],
                        k=tri_iso.simplices[:, 2],
                        opacity=0.3,
                        name=f'qc ‚â• {qc_val:.1f} MPa'
                    ))

        fig_isosurface.update_layout(
            title="Iso-surfaces 3D Triangul√©es des Param√®tres G√©otechniques",
            scene=dict(
                xaxis_title="Position X (m)",
                yaxis_title="Profondeur (m)",
                zaxis_title="qc (MPa)"
            )
        )
        visualizations['isosurface_3d'] = fig_isosurface

    except Exception as e:
        st.warning(f"Erreur lors des isosurfaces: {e}")

    # 19. R√©seau 3D interconnect√© des zones
    try:
        fig_network_3d = go.Figure()

        # Cr√©er des connexions entre zones proches
        for i in range(len(zone_centers)):
            for j in range(i+1, len(zone_centers)):
                dist = np.sqrt((zone_centers[i][0] - zone_centers[j][0])**2 +
                              (zone_centers[i][1] - zone_centers[j][1])**2)
                if dist < 80:  # Distance maximale pour connexion
                    fig_network_3d.add_trace(go.Scatter3d(
                        x=[zone_centers[i][0], zone_centers[j][0]],
                        y=[zone_centers[i][1], zone_centers[j][1]],
                        z=[0, 0],  # √Ä la surface
                        mode='lines',
                        line=dict(color='gray', width=2),
                        name=f'Connexion {i+1}-{j+1}'
                    ))

        # Ajouter les zones comme points
        for idx, (x, y) in enumerate(zone_centers):
            zone_data = df_zones[df_zones['zone_id'] == idx]
            avg_qc = zone_data['qc'].mean() if not zone_data.empty else 0

            fig_network_3d.add_trace(go.Scatter3d(
                x=[x],
                y=[y],
                z=[0],
                mode='markers+text',
                marker=dict(size=15, color=avg_qc, colorscale='Viridis', showscale=True),
                text=[f'Zone {idx+1}'],
                textposition="top center",
                name=f'Zone {idx+1}'
            ))

        fig_network_3d.update_layout(
            title="R√©seau 3D Interconnect√© des Zones CPTU",
            scene=dict(
                xaxis_title="Position X (m)",
                yaxis_title="Position Y (m)",
                zaxis_title="Surface"
            )
        )
        visualizations['network_zones_3d'] = fig_network_3d

    except Exception as e:
        st.warning(f"Erreur lors du r√©seau 3D: {e}")

    # 20. Toiles d'araign√©e (Spider plots) pour les propri√©t√©s par zone
    try:
        fig_spider_zones = go.Figure()

        # Propri√©t√©s √† analyser
        properties = ['qc', 'fs', 'Ic', 'Fr']
        if 'FS_Liquefaction' in df.columns:
            properties.append('FS_Liquefaction')

        # Normaliser les valeurs pour chaque propri√©t√©
        normalized_data = {}
        for prop in properties:
            if prop in df.columns:
                values = df[prop].values
                normalized_data[prop] = (values - values.min()) / (values.max() - values.min())

        # Cr√©er une toile par zone
        for zone_id in range(min(5, n_zones)):  # Maximum 5 toiles pour lisibilit√©
            zone_data = df_zones[df_zones['zone_id'] == zone_id]

            if not zone_data.empty:
                r_values = []
                for prop in properties:
                    if prop in normalized_data:
                        zone_values = normalized_data[prop][zone_data.index]
                        r_values.append(zone_values.mean())
                    else:
                        r_values.append(0)

                # Ajouter les valeurs de d√©but et fin pour fermer la toile
                r_values.append(r_values[0])
                theta_values = properties + [properties[0]]

                fig_spider_zones.add_trace(go.Scatterpolar(
                    r=r_values,
                    theta=theta_values,
                    fill='toself',
                    name=f'Zone {zone_id + 1}',
                    opacity=0.7
                ))

        fig_spider_zones.update_layout(
            title="Toiles d'Araign√©e des Propri√©t√©s G√©otechniques par Zone",
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            showlegend=True
        )
        visualizations['spider_zones'] = fig_spider_zones

    except Exception as e:
        st.warning(f"Erreur lors des toiles d'araign√©e: {e}")

    return visualizations


class GeotechnicalAnalyzer:
    """Classe pour effectuer l'analyse g√©otechnique compl√®te"""

    def __init__(self):
        self.analysis_methods = {
            'soil_classification': ['Robertson (1986)', 'Schmertmann (1978)'],
            'liquefaction': ['NCEER (1997)', 'Cetin et al. (2004)']
        }

    def analyze_cpt_data(self, df, groundwater_level=2.0, soil_classification_method='Robertson (1986)',
                        liquefaction_method='NCEER (1997)'):
        """Effectue une analyse compl√®te des donn√©es CPT"""
        try:
            results = {}

            # Classification des sols
            df_soil = estimate_soil_type(df.copy())
            results['soil_classification'] = df_soil

            # Analyse de liqu√©faction
            df_crr = calculate_crr(df_soil.copy())
            results['liquefaction_analysis'] = df_crr

            # Clustering (sans composants Streamlit)
            from models.clustering import perform_clustering
            df_clustered, kmeans, scaler, pca = perform_clustering(df_crr, n_clusters=3)

            # Si le clustering √©choue, utiliser les donn√©es CRR
            if df_clustered is None:
                df_clustered = df_crr
                models = None
            else:
                models = {
                    'kmeans': kmeans,
                    'scaler': scaler,
                    'pca': pca
                }

            # M√©triques g√©n√©rales
            results['dominant_soil_type'] = df_clustered['Soil_Type'].mode().iloc[0] if 'Soil_Type' in df_clustered.columns else 'Unknown'
            results['liquefaction_risk'] = self._assess_liquefaction_risk(df_clustered)
            results['critical_depth'] = df_clustered['Depth'].max()
            results['safety_factor'] = self._calculate_safety_factor(df_clustered)

            # Stocker les donn√©es analys√©es et les mod√®les
            results['analyzed_data'] = df_clustered
            results['models'] = models

            return results

        except Exception as e:
            raise ValueError(f"Erreur lors de l'analyse g√©otechnique: {str(e)}")

    def _assess_liquefaction_risk(self, df):
        """√âvalue le risque de liqu√©faction global"""
        if 'CRR' not in df.columns:
            return 'Non √©valu√©'

        crr_values = df['CRR'].dropna()
        if len(crr_values) == 0:
            return 'Non √©valu√©'

        avg_crr = crr_values.mean()
        if avg_crr < 0.1:
            return '√âlev√©'
        elif avg_crr < 0.3:
            return 'Mod√©r√©'
        else:
            return 'Faible'

    def _calculate_safety_factor(self, df):
        """Calcule le facteur de s√©curit√© moyen"""
        if 'CRR' not in df.columns:
            return 0.0

        crr_values = df['CRR'].dropna()
        if len(crr_values) == 0:
            return 0.0

        return crr_values.mean()

def create_correlation_matrix(df):
    """Cr√©e un tableau de corr√©lation complet entre toutes les propri√©t√©s g√©otechniques"""
    try:
        # S√©lectionner les colonnes num√©riques pertinentes pour la corr√©lation
        numeric_columns = []
        correlation_columns = []

        # Colonnes de base
        base_columns = ['Depth', 'qc', 'fs', 'Ic']
        for col in base_columns:
            if col in df.columns:
                numeric_columns.append(col)
                correlation_columns.append(col)

        # Colonnes d√©riv√©es de l'analyse
        derived_columns = ['CRR', 'FS_Liquefaction', 'qc_smooth', 'fs_smooth', 'Ic_smooth',
                          'Friction_Ratio', 'Soil_Density', 'Young_Modulus', 'qc_gradient',
                          'cluster_distance', 'pca_1', 'pca_2']

        for col in derived_columns:
            if col in df.columns and df[col].dtype in ['int64', 'float64']:
                numeric_columns.append(col)
                correlation_columns.append(col)

        if len(numeric_columns) < 2:
            st.warning("Pas assez de colonnes num√©riques pour calculer la corr√©lation")
            return None

        # Calculer la matrice de corr√©lation
        correlation_matrix = df[numeric_columns].corr()

        # Cr√©er une figure Plotly pour la matrice de corr√©lation
        fig_correlation = go.Figure(data=go.Heatmap(
            z=correlation_matrix.values,
            x=correlation_matrix.columns,
            y=correlation_matrix.columns,
            colorscale='RdBu',
            zmin=-1,
            zmax=1,
            text=np.round(correlation_matrix.values, 2),
            texttemplate='%{text}',
            textfont={"size": 10},
            hoverongaps=False
        ))

        fig_correlation.update_layout(
            title="Matrice de Corr√©lation Compl√®te des Propri√©t√©s G√©otechniques",
            xaxis_title="Propri√©t√©s",
            yaxis_title="Propri√©t√©s",
            width=800,
            height=800,
            xaxis=dict(tickangle=-45),
            yaxis=dict(tickangle=0)
        )

        # Cr√©er aussi un tableau stylis√© avec les valeurs num√©riques
        correlation_table = correlation_matrix.round(3)

        # Ajouter des annotations pour interpr√©ter les corr√©lations
        annotations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(len(correlation_matrix.columns)):
                if i != j:  # Ne pas annoter la diagonale
                    corr_value = correlation_matrix.iloc[i, j]
                    if abs(corr_value) > 0.7:
                        strength = "Forte"
                        color = "red" if corr_value > 0 else "blue"
                    elif abs(corr_value) > 0.5:
                        strength = "Mod√©r√©e"
                        color = "orange" if corr_value > 0 else "cyan"
                    elif abs(corr_value) > 0.3:
                        strength = "Faible"
                        color = "yellow" if corr_value > 0 else "lightblue"
                    else:
                        strength = "Tr√®s faible"
                        color = "white"

                    annotations.append({
                        'x': correlation_matrix.columns[j],
                        'y': correlation_matrix.columns[i],
                        'text': f"{strength}<br>({corr_value:.2f})",
                        'showarrow': False,
                        'font': {'size': 8, 'color': 'black'},
                        'bgcolor': color,
                        'opacity': 0.8
                    })

        # Cr√©er une version annot√©e de la heatmap
        fig_correlation_annotated = go.Figure(data=go.Heatmap(
            z=correlation_matrix.values,
            x=correlation_matrix.columns,
            y=correlation_matrix.columns,
            colorscale='RdBu',
            zmin=-1,
            zmax=1,
            text=np.round(correlation_matrix.values, 2),
            texttemplate='%{text}',
            textfont={"size": 10},
            hoverongaps=False
        ))

        # Ajouter les annotations
        for annotation in annotations:
            fig_correlation_annotated.add_annotation(
                x=annotation['x'],
                y=annotation['y'],
                text=annotation['text'],
                showarrow=annotation['showarrow'],
                font=annotation['font'],
                bgcolor=annotation['bgcolor'],
                opacity=annotation['opacity']
            )

        fig_correlation_annotated.update_layout(
            title="Matrice de Corr√©lation Annot√©e des Propri√©t√©s G√©otechniques",
            xaxis_title="Propri√©t√©s",
            yaxis_title="Propri√©t√©s",
            width=900,
            height=900,
            xaxis=dict(tickangle=-45),
            yaxis=dict(tickangle=0)
        )

        # Statistiques descriptives des corr√©lations
        corr_stats = {
            'max_correlation': correlation_matrix.max().max(),
            'min_correlation': correlation_matrix.min().min(),
            'strong_positive_corr': len(correlation_matrix[(correlation_matrix > 0.7) & (correlation_matrix < 1.0)].stack()),
            'strong_negative_corr': len(correlation_matrix[(correlation_matrix < -0.7)].stack()),
            'columns_analyzed': len(numeric_columns)
        }

        return {
            'correlation_matrix': correlation_table,
            'correlation_heatmap': fig_correlation,
            'correlation_annotated': fig_correlation_annotated,
            'correlation_stats': corr_stats,
            'analyzed_columns': numeric_columns
        }

    except Exception as e:
        st.warning(f"Erreur lors de la cr√©ation de la matrice de corr√©lation: {e}")
        return None


def perform_complete_analysis(df, n_clusters=3, use_streamlit=True):
    """Effectue une analyse compl√®te et avanc√©e CPTU avec 3D et g√©olocalisation"""
    try:
        if use_streamlit:
            st.info("üîÑ Analyse compl√®te avanc√©e en cours...")

        # √âtape 1: Classification d√©taill√©e des sols
        if use_streamlit:
            with st.spinner("üå± √âtape 1/5: Classification d√©taill√©e des sols..."):
                df_analyzed = estimate_soil_type(df)
                if df_analyzed is None:
                    raise ValueError("√âchec de la classification des sols")
                progress_bar = st.progress(20)
                st.success("‚úÖ Classification d√©taill√©e des sols termin√©e!")
        else:
            df_analyzed = estimate_soil_type(df)
            if df_analyzed is None:
                raise ValueError("√âchec de la classification des sols")


        # √âtape 2: Calcul avanc√© du CRR et liqu√©faction
        if use_streamlit:
            with st.spinner("üåä √âtape 2/5: Analyse de liqu√©faction avanc√©e..."):
                df_crr = calculate_crr(df_analyzed)
                if df_crr is None:
                    raise ValueError("√âchec du calcul du CRR")
                progress_bar.progress(40)
                st.success("‚úÖ Analyse de liqu√©faction termin√©e!")
        else:
            df_crr = calculate_crr(df_analyzed)
            if df_crr is None:
                raise ValueError("√âchec du calcul du CRR")

        # √âtape 3: Identification des couches 3D
        if use_streamlit:
            with st.spinner("üèîÔ∏è √âtape 3/5: Identification des couches g√©ologiques 3D..."):
                layers_df = identify_soil_layers_3d(df_crr)
                progress_bar.progress(60)
                st.success(f"‚úÖ {len(layers_df)} couches g√©ologiques identifi√©es!")
        else:
            layers_df = identify_soil_layers_3d(df_crr)

        # √âtape 4: Analyse g√©ospatiale
        if use_streamlit:
            with st.spinner("üåç √âtape 4/5: G√©olocalisation des points..."):
                gdf = create_geospatial_analysis(df_crr)
                progress_bar.progress(80)
                if gdf is not None:
                    st.success("‚úÖ G√©olocalisation termin√©e!")
                else:
                    st.warning("‚ö†Ô∏è G√©olocalisation limit√©e (GeoPandas non disponible)")
        else:
            gdf = create_geospatial_analysis(df_crr)

        # √âtape 5: Clustering avanc√©
        if use_streamlit:
            with st.spinner("üéØ √âtape 5/5: Clustering automatique avanc√©..."):
                from models.clustering import perform_clustering
                df_clustered, kmeans, scaler, pca = perform_clustering(df_crr, n_clusters)
                if df_clustered is None:
                    raise ValueError("√âchec du clustering")
                progress_bar.progress(100)
                st.success("‚úÖ Clustering avanc√© termin√©!")
        else:
            from models.clustering import perform_clustering
            df_clustered, kmeans, scaler, pca = perform_clustering(df_crr, n_clusters)
            if df_clustered is None:
                raise ValueError("√âchec du clustering")

        # Cr√©ation des visualisations avanc√©es
        if use_streamlit:
            with st.spinner("üìä G√©n√©ration des graphiques avanc√©s..."):
                visualizations = create_advanced_visualizations(df_clustered, layers_df, gdf)
        else:
            visualizations = create_advanced_visualizations(df_clustered, layers_df, gdf)

        # Cr√©ation du tableau de corr√©lation complet
        if use_streamlit:
            with st.spinner("üìà Calcul du tableau de corr√©lation complet..."):
                correlation_results = create_correlation_matrix(df_clustered)
                if correlation_results:
                    st.success("‚úÖ Tableau de corr√©lation g√©n√©r√©!")
                else:
                    st.warning("‚ö†Ô∏è Impossible de g√©n√©rer le tableau de corr√©lation")
        else:
            correlation_results = create_correlation_matrix(df_clustered)

        # Sauvegarder les mod√®les et r√©sultats
        models = {
            'kmeans': kmeans,
            'scaler': scaler,
            'pca': pca
        }

        results = {
            'data': df_clustered,
            'layers': layers_df,
            'geospatial': gdf,
            'models': models,
            'visualizations': visualizations,
            'correlation_analysis': correlation_results
        }

        if use_streamlit:
            progress_bar.empty()
            st.success("üéâ Analyse compl√®te avanc√©e termin√©e avec succ√®s!")
            st.info(f"üìä {len(visualizations)} graphiques avanc√©s g√©n√©r√©s")

        return df_clustered, models, results

    except Exception as e:
        if use_streamlit:
            st.error(f"‚ùå Erreur lors de l'analyse compl√®te: {str(e)}")
            st.error("Retour aux donn√©es brutes...")
        return df, None, None